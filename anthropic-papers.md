# The Anthropic Application Papers

The anthropic alignment team lists these papers as similar to what the team works on.

If I'm remotely serious about working on alignment, I should be able to read and grok them.

- [Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training](https://arxiv.org/abs/2401.05566)
- [Studying Large Language Model Generalization with Influence Functions](https://arxiv.org/abs/2308.03296)
- [Language Models (Mostly) Know What They Know](https://arxiv.org/abs/2207.05221)
- [Measuring Progress on Scalable Oversight for Large Language Models](https://arxiv.org/abs/2211.03540)
- [Measuring Faithfulness in Chain-of-Thought Reasoning](https://arxiv.org/abs/2307.13702)
- [Discovering Language Model Behaviors with Model-Written Evaluations](https://arxiv.org/abs/2212.09251)
- [Debating with More Persuasive LLMs Leads to More Truthful Answers](https://raw.githubusercontent.com/ucl-dark/llm_debate/main/paper.pdf)


